# Probability

For the following section, $A$ and $B$ represent events in the sample space $S$.

## Axioms

1. $\mathbb{P}(A) \geq 0 \quad \forall A \subset S$
2. $\mathbb{P}(S) = 1$
3. If $A \cap B = \emptyset$, then $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B)$

## Union Rule

$\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$

## Inclusion-Exclusion Principle

$\mathbb{P}(A \cup B \cup C) = \mathbb{P}(A) + \mathbb{P}(B) + \mathbb{P}(C) - \mathbb{P}(A \cap B) - \mathbb{P}(A \cap B) - \mathbb{P}(B \cap C) + \mathbb{P}(A \cap B \cap C)$

## De Morgan's Laws

$(A \cup B)^c = A^c \cap B^c$

$(A \cap B)^c = A^c \cup B^c$

## Conditional Probability

$\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$

$\mathbb{P}(A) = \mathbb{P}(A|B)\mathbb{P}(B) + \mathbb{P}(A|B^c)\mathbb{P}(B^c)$

## Bayes' Theorem

$\mathbb{P}(B_j|A) = \frac{\mathbb{P}(A|B_j)\mathbb{P}(B_j)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A|B_j)\mathbb{P}(B_j)}{\sum_{i=1}^k \mathbb{P}(A|B_i)\mathbb{P}(B_i)}$

## Independence

If events $A$ and $B$ are independent: $\mathbb{P}(A|B) = \mathbb{P}(A)$

## Counting Examples

- There are $n!$ ways to arrange $n$ distinct elements in an ordered list.
- There are $6^n$ outcomes for $n$ tosses of a $6$-sided die.