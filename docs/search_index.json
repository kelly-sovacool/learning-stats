[
["index.html", "Stats Reference Preface", " Stats Reference Kelly Sovacool 2018 Preface A reference for concepts &amp; equations in introductory statistics. The source code is hosted on GitHub. I used Sean Kross’ bookdown starter kit to begin creating this R Markdown book. "],
["probability.html", "1 Probability 1.1 Axioms 1.2 Union Rule 1.3 Conditional Probability 1.4 Bayes’ Theorem", " 1 Probability 1.1 Axioms 1.2 Union Rule 1.3 Conditional Probability 1.4 Bayes’ Theorem "],
["distributions-of-random-variables.html", "2 Distributions of Random Variables 2.1 Discrete 2.2 Continuous 2.3 Properties of Density Functions 2.4 Properties of Distribution Functions 2.5 Parameters 2.6 Hazard &amp; Survival", " 2 Distributions of Random Variables 2.1 Discrete CDF: \\(\\displaystyle\\sum_{k=0}^i p(k)\\) 2.1.1 Bernoulli \\(X\\sim\\text{Bern}(p)\\) \\(\\mathbb{E}[X] = p\\) \\(\\text{Var}[X] = p(1-p)\\) \\[\\begin{equation}\\begin{aligned} p(x) = \\begin{cases} p &amp; x=1 \\\\ 1-p &amp; x=0 \\\\ 0 &amp; else \\end{cases}\\end{aligned}\\end{equation}\\] 2.1.2 Binomial \\(X\\sim\\text{Binom}(n,p)\\) \\(\\mathbb{E}[X] = np\\) \\(\\text{Var}[X] = np(1-p)\\) \\[p(k) = \\binom{n}{k} p^k (1-p)^{n-k}\\] 2.1.3 Poisson \\(X\\sim\\text{Poisson}(\\lambda\\sim np)\\) \\(\\mathbb{E}[X] = \\lambda\\) \\(\\text{Var}[X] = \\lambda\\) \\[p(k) = e^{-\\lambda} \\frac{\\lambda^k}{k!}\\] Approximation to binomial when \\(n \\rightarrow \\infty\\) and \\(p \\rightarrow 0\\). E.g. number of misprints per page of a book. 2.1.4 Geometric \\(X\\sim\\text{Geom}(p)\\) \\(\\mathbb{E}[X] = \\frac{1}{p}\\) \\(\\text{Var}[X] = \\frac{1-p}{p^2}\\) \\[\\begin{equation}\\begin{aligned} p(k) &amp;= (1-p)^{k-1} \\\\ \\\\ F(k) &amp;= 1-(1-p)^k \\\\ \\end{aligned}\\end{equation}\\] Experiment with infinite trials; stop at first success. Memoryless. E.g. flip a coin until heads comes up. 2.2 Continuous 2.2.1 Uniform \\(X\\sim\\text{Unif}(a,b)\\) \\(\\mathbb{E}[X] = \\frac{b+a}{2}\\) \\(\\text{Var}[X] = \\frac{(b-a)^2}{12}\\) \\[\\begin{equation}\\begin{aligned} f(x) &amp;= \\begin{cases} \\frac{1}{b-a} &amp; x \\in [a,b] \\\\ 0 &amp; \\text{else} \\\\ \\end{cases} \\\\ \\\\ F(x) &amp;= \\begin{cases} \\frac{x-a}{b-a} &amp; x \\in [a,b] \\\\ 0 &amp; \\text{else} \\end{cases}\\end{aligned}\\end{equation}\\] Simplest continuous distribution. All outcomes equally likely. E.g. pick random point on disk of radius \\(r\\). \\(x\\) is distance to center. \\(f(x) = \\frac{2x}{r^2}\\) when \\(0 \\leq x \\leq r\\). 2.2.2 General Normal \\(X\\sim\\text{N}(\\mu,\\sigma)\\) \\(\\mathbb{E}[X] = \\mu\\) \\(\\text{Var}[X] = \\sigma^2\\) \\[\\begin{equation}\\begin{aligned} f(x) &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-(x-\\mu)^2/2\\sigma^2} \\\\ \\\\ F(x) &amp;= \\phi(Z=\\frac{x-\\mu}{\\sigma}) \\end{aligned}\\end{equation}\\] To find CDF, convert to standard normal, then use Z table. E.g. biological variables. E.g. exam scores. 2.2.2.1 Normal Approximation to the Binomial Distribution When \\(X\\sim\\text{Binom}(n,p)\\), \\(n \\rightarrow \\infty\\), &amp; \\(p \\rightarrow \\frac{1}{2}\\): \\(\\mathbb{E}[X] = np = \\mu\\), \\(\\sigma = \\sqrt{np(1-p)}\\), \\(z = \\frac{x-np}{\\sqrt{np(1-p)}}\\) $F_z(a) (a) $, \\(\\mathbb{P}(a \\leq z \\leq b) \\approx \\phi (b) - \\phi (a)\\) via De Maivre-Laplace Central Limit Theorem 2.2.3 Standard Normal \\(X\\sim\\text{N}(0,1)\\) \\(\\mathbb{E}[X] = 0\\) \\(\\text{Var}[X] = 1\\) \\[\\begin{equation}\\begin{aligned} f(x) &amp;= \\frac{1}{\\sqrt{2\\pi}} e^{-(x^2)/2} \\\\ \\\\ F(x) &amp;= \\phi(x) \\end{aligned}\\end{equation}\\] To find CDF, use Z table. Special case of the normal with no parameters. 2.2.4 Exponential \\(X\\sim\\text{Exp}(\\lambda)\\) \\(\\mathbb{E}[X] = \\frac{1}{\\lambda}\\) \\(\\text{Var}[X] = \\frac{1}{\\lambda^2}\\) \\[\\begin{equation}\\begin{aligned} f(x) &amp;= \\begin{cases} \\lambda e^{-\\lambda x} &amp; x &gt; 0 \\\\ 0 &amp; else \\\\ \\end{cases} \\\\ \\\\ F(x) &amp;= \\begin{cases} 1-e^{-\\lambda x} &amp; x &gt; 0 \\\\ 0 &amp; else \\end{cases} \\end{aligned}\\end{equation}\\] Memoryless. \\(\\lambda=\\) rate. Continuous version of Geom(\\(p\\)). 2.2.5 Gamma \\(X\\sim\\Gamma[\\alpha,\\lambda]\\) \\(\\mathbb{E}[X] = \\frac{\\alpha}{\\lambda}\\) \\(\\text{Var}[X] = \\frac{\\alpha}{\\lambda^2}\\) 2.2.6 Chi Square \\(X\\sim\\chi^2[n]\\) \\(\\mathbb{E}[X] = n\\) \\(\\text{Var}[X] = 2n\\) Special case of \\(\\Gamma\\) where \\(\\alpha=\\frac{n}{2}\\) and \\(\\lambda=\\frac{1}{2}\\). 2.3 Properties of Density Functions 2.4 Properties of Distribution Functions 2.5 Parameters Discrete: \\(\\mathbb{E}[X] = \\mu = \\sum_{i=1}^k x_i p_i\\) Continuous: \\(\\mathbb{E}[X] = \\mu = \\int_{-\\infty}^{\\infty} x f(x) dx\\) Law of total expectation: \\(\\mathbb{E}[X] = \\sum_j \\mathbb{E}(E|F_j)\\mathbb{P}(F_j)\\) \\(\\text{Var}[X] = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 = \\sigma^2\\) \\(\\sigma = \\sqrt{\\text{Var}[X]}\\) \\(Z = \\frac{x-\\mu}{\\sigma}\\), \\(Z\\sim~\\text{N}(0,1)\\) 2.6 Hazard &amp; Survival Survival: \\(S_T(t) = \\mathbb{P}(T&gt;t) = 1 - \\mathbb{P}(T \\leq t) = 1 - F_T(t) = e^{-\\int_{u=o}^t \\lambda(u) du}\\) Density: \\(f_T(t) = F_T&#39;(t) = -S_T&#39;(t)\\) Hazard: \\(\\lambda(t) = \\frac{f_T(t)}{S_T(t)} = \\frac{-S_T&#39;(t)}{S_T(t)} = -\\frac{d}{dt} \\log S_T(t)\\) "],
["joint-distributions.html", "3 Joint Distributions 3.1 Marginals 3.2 Independence 3.3 Min/Max", " 3 Joint Distributions 3.1 Marginals 3.2 Independence 3.3 Min/Max "],
["sums-of-independent-random-variables.html", "4 Sums of Independent Random Variables 4.1 Uniform 4.2 Normal 4.3 Gamma 4.4 Poisson 4.5 Binomial 4.6 Geometric", " 4 Sums of Independent Random Variables Convolution (CDF): \\(F_{X+Y}(a) = \\mathbb{P}(X+Y \\leq a) = \\int_{-\\infty}^\\infty F_X (a-y) f_Y (y) dy\\) Density (PDF): \\(f_{X+Y} = \\int_{-\\infty}^\\infty f_X (a-y) f_Y (y) dy\\) 4.1 Uniform 4.2 Normal The sum of \\(n\\) normal RVs \\(\\sum_i^n X_i\\) is normally distributed with parameters: \\(\\mu = \\sum_i^n \\mu_i\\) \\(\\sigma^2 = \\sum_i^n \\sigma_i^2\\) \\(\\sigma = \\sqrt{\\sum_i^n \\sigma_i^2} \\neq \\sum_i^n \\sqrt{\\sigma_i^2}\\) 4.3 Gamma 4.4 Poisson \\(X_1\\sim\\text{Poisson}(\\lambda_1)\\) \\(X_2\\sim\\text{Poisson}(\\lambda_2)\\) \\(Y = X_1+Y_2\\) \\(Y\\sim\\text{Poisson}(\\lambda= \\lambda_1 + \\lambda_2)\\) \\(\\mathbb{P}(X_1+X_2=n) = \\frac{1}{n!} e^{-(\\lambda_1+\\lambda_2)} (\\lambda_1 + \\lambda_2)^n\\) 4.5 Binomial \\(X_1\\sim\\text{Binom}(n,p)\\) \\(X_2\\sim\\text{Binom}(m,p)\\) \\(Y = X_1+Y_2\\) \\(Y\\sim\\text{Binom}(n+m,p)\\) \\(\\mathbb{P}(X_1+X_2=k) = \\binom{n+m}{k} = \\sum_{i=0}^n \\binom{n}{i} \\binom{m}{k-i}\\) 4.6 Geometric "],
["calculus-review.html", "5 Calculus Review 5.1 Central Limit Theorem 5.2 Derivative &amp; Integration rules", " 5 Calculus Review 5.1 Central Limit Theorem 5.2 Derivative &amp; Integration rules 5.2.1 Integration by substitution 5.2.2 Integration by parts "]
]
